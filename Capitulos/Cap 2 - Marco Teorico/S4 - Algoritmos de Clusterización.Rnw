\subsection{Algoritmos de Agrupamiento}

Una vez que se determina la medida de disimilitud, se obtiene una matriz de disimilitud inicial (que contiene la disimilitud entre parejas de series), y luego se usa un algoritmo de agrupamiento convencional para formar los clústers (grupos) con las series. De hecho, la mayoría de los enfoques de agrupamiento de series de tiempo revisados por \cite{liao2005clustering} son variaciones de procedimientos generales como por ejemplo: K-Means, K-Medoids, PAM, CLARA \cite{kaufman1986clustering} o de Clúster jerárquico que utilizan una gama de disimilitudes específicamente diseñadas para tratar con series de tiempo y algunas de sus características. 

\subsubsection{Particionamiento alrededor de Medoides (PAM)}

El algoritmo PAM fue propuesto en \cite{rousseeuw1990finding}, tiene por objetivo hallar $k$ grupos (clústers), esto mediante la identificación de objetos representativos que están lo más centralmente localizados dentro de cada grupo, estos objetos se conocen como "medoides". Una vez identificados los medoides, los objetos se agrupan al medoide más similar.

%Por ejemplo si un objeto $O_i$ es el medoide de un grupo, entonces diremos que el objeto $O_j$ pertenece a dicho grupo si $d(O_j , O_i) = \underset{O_m}{\min} \{ d(O_j,O_m) \}$

\textbf{Observación. } La "calidad de agrupamiento" del método se mide como la distancia promedio entre los objetos y sus respectivos medoides.

La manera en la que PAM halla los $k$ medoides es partir de un conjunto arbitrario de objetos para luego intercambiarlos sucesivamente de tal manera de que en cada paso se mejore la calidad de agrupamiento.

Por ejemplo, para medir el efecto de intercambiar un objeto $O_i_1$ por $O_i_2$ el algoritmo PAM calcula el costo $C_j (i_1,i_2)$ (para todo objeto $O_j$ no selecionado. $C_j (i_1,i_2)$  se calcula según cada uno de los siguientes casos:


\begin{enumerate}

\item  Supongamos que $O_j$ pertenece al grupo representado por el medoide $O_i$. Luego supongamos que $O_j$ es más parecido a $O_{k}$ que $O_h$. Así, si reemplazamos $O_i$ por $O_h$ como medoide del grupo, entonces $O_j$ pertenecería al grupo representado por $O_{k}$. Por lo tanto el costo de intercambio de medoides respecto de $O_j$ es :

$$C_{j}(i,h)= d(O_j,O_{k})-d(O_j,O_i)$$

Notemos que $C_{j}(i,h)\geq 0$ 

\item Supongamos que $O_j$ pertenece al grupo representado por el medoide $O_i$. Pero esta vez $O_j$ es menos parecido a $O_{k}$ que $O_h$. Así, el costo de  reemplazar $O_i$ por $O_h$ viene dado por:

$$C_{j}(i,h)= d(O_j,O_{h})-d(O_j,O_i)$$

En este caso $C_{j}(i,h)$ puede ser positivo o negativo.

\item Supongamos que $O_j$ pertenece a un grupo distinto al representado por el medoide $O_i$ . Sea $O_{k}$ el medoide de ese grupo. Luego supongamos que $O_j$ es más similar a $O_{k}$ que a $O_h$, entonces:

$$C_{j}(i,h)= 0$

\item Supongamos que $O_j$ pertenece al grupo representado por el medoide $O_i$. Entonces reeplazar $O_i$ con $O_h$ provocaría que $O_j$ pase del grupo representado por $O_h$ al grupo representado por $O_k$. Así, el costo viene dado por:

$$C_{j}(i,h) = d(O_j,O_{h})-d(O_j,O_k)$$

Notemos que $C_{j}(i,h)<0$

\item Finalmente el costo total de reemplazar $O_i$ por $O_h$ está dado por:

$$T(i,h)= \sum_{i} C_{j}(i,h)$$

\end{enumerate} 


\textbf{Algoritmo}

\begin{enumerate}
\item Seleccionar $k$ objetos arbitrariamente
\item Calcular $T(i,h)$ para todos los pares de objetos, tales que $O_i$ está seleccionado y $O_h$ no.
\item Seleccionar el par $O_i, O_h$ que minimice $T(i,h)$. Si el mínimo $T(i,h)$ es negativo, reemplazar $O_i$ con $O_h$ y vuelva al paso 2. 
\item Caso contrario, para cada objeto no seleccionado, hallar el medoide más parecido.  
\end{enumerate}

\textbf{Nota.} Resultados experimentales muestran que PAM funciona adecuadamente con conjuntos de datos pequeños (100 objetos), pero no es eficiente para grandes conjuntos de datos, lo que es evidente al analizar la complejidad del algoritmo PAM donde vemos que cada iteración del algoritmo tiene un orden de complejidad de $O(k(n-k)^2)$.

\subsubsection{CLARA}
CLARA (Clustering Large Aplications) es un método desarrollado por Kaufman y Rousseuw con la finalidad de lidiar con un gran número de datos. El algoritmo CLARA consiste básicamente  en aplicar PAM sobre una muestra aleatoria de objetos, en lugar de aplicarlo a todos los objetos. Esto debido a que los medoides de una muestra aproximaría a los medoides de todos los objetos. Para mejorar esta aproximación CLARA toma varias muestras y devuelve la mejor agrupación. En este caso, la calidad de agrupamiento se mide como la distancia promedio entre todos los objetos y sus medoides (no solo los de la muestra).

\textbf{Algoritmo}

\begin{enumerate}
\item Para $i$ de $1$ a $L$ realizar:
\item Tomar una muestra de $m$ objetos aleatoriamente, y ejecutar el algoritmo PAM para hallar los $k$ medoides de la muestra.
\item Para cada objeto $O_j$ en la data entera, determinar cual de los $k$ medoides es el más similar.
\item Calcular la distancia (o disimilitud) promedio del agrupamiento obtenido en el paso anterior. Si este valor es menor al mínimo anterior, actualizamos el valor mínimo y guardar los $k$ medoides del paso 2 como los mejores medoides obtenidos hasta el momento.

\end{enumerate}

Corridas experimentales realizadas en \cite{rousseeuw1990finding} muestran que tomar $L=5$ muestras de tamaño $m=40 + 2k$ da buenos resultados. 

\textbf{Observación. } Se puede corroborar que el orden de complejidad del algoritmo CLARA es $O(k(40+k)^2+k(n-k))$, esto explica porque CLARA es más eficiente que PAM para valores grandes de $n$.


%\subsection{CLARANS}

\subsection{Validación}

Una etapa adicional dentro del análisis clúster consiste en determinar la cantidad de clústers que es más apropiada para los datos. Idealmente, los clústers resultantes no solo deberían tener buenas propiedades estadísticas (compactas, bien separadas, conectadas y estables), sino también resultados relevantes. Se han propuesto una variedad de medidas y métodos para validar los resultados de un análisis clúster y determinar tanto el número de clústers, así como identificar qué algoritmo de agrupamiento ofrece el mejor rendimiento, algunas de estas ellas pueden encontrarse en \cite{fraley1998many}; \cite{duda2001pattern} ; \cite{salvador2004determining} ; \cite{kerr2001bootstrapping}. Esta validación puede basarse únicamente en las propiedades internas de los datos o en alguna referencia externa.





